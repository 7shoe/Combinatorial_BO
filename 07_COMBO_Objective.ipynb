{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "82b58d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "48e4f925",
   "metadata": {},
   "outputs": [],
   "source": [
    "ISING_GRID_H = 4\n",
    "ISING_GRID_W = 4\n",
    "ISING_N_EDGES = 24\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "23641968",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spin_covariance(interaction, grid_shape):\n",
    "    horizontal_interaction, vertical_interaction = interaction\n",
    "    n_vars = horizontal_interaction.shape[0] * vertical_interaction.shape[1]\n",
    "    spin_cfgs = np.array(list(itertools.product(*([[-1, 1]] * n_vars))))\n",
    "    density = np.zeros(spin_cfgs.shape[0])\n",
    "    for i in range(spin_cfgs.shape[0]):\n",
    "        spin_cfg = spin_cfgs[i].reshape(grid_shape)\n",
    "        h_comp = spin_cfg[:, :-1] * horizontal_interaction * spin_cfg[:, 1:] * 2\n",
    "        v_comp = spin_cfg[:-1] * vertical_interaction * spin_cfg[1:] * 2\n",
    "        log_interaction_energy = np.sum(h_comp) + np.sum(v_comp)\n",
    "        density[i] = np.exp(log_interaction_energy)\n",
    "    interaction_partition = np.sum(density)\n",
    "    density = density / interaction_partition\n",
    "\n",
    "    covariance = spin_cfgs.T.dot(spin_cfgs * density.reshape((-1, 1)))\n",
    "    return covariance, interaction_partition\n",
    "\n",
    "def sample_init_points(n_vertices, n_points, random_seed=None):\n",
    "    \"\"\"\n",
    "\n",
    "    :param n_vertices: 1D array\n",
    "    :param n_points:\n",
    "    :param random_seed:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if random_seed is not None:\n",
    "        rng_state = torch.get_rng_state()\n",
    "        torch.manual_seed(random_seed)\n",
    "    init_points = torch.empty(0).long()\n",
    "    for _ in range(n_points):\n",
    "        init_points = torch.cat([init_points, torch.cat([torch.randint(0, int(elm), (1, 1)) for elm in n_vertices], dim=1)], dim=0)\n",
    "    if random_seed is not None:\n",
    "        torch.set_rng_state(rng_state)\n",
    "    return init_points\n",
    "\n",
    "def generate_ising_interaction(grid_h, grid_w, random_seed=None):\n",
    "    if random_seed is not None:\n",
    "        rng_state = torch.get_rng_state()\n",
    "        torch.manual_seed(random_seed)\n",
    "    horizontal_interaction = ((torch.randint(0, 2, (grid_h * (grid_w - 1), )) * 2 - 1).float() * (torch.rand(grid_h * (grid_w - 1)) * (5 - 0.05) + 0.05)).view(grid_h, grid_w-1)\n",
    "    vertical_interaction = ((torch.randint(0, 2, ((grid_h - 1) * grid_w, )) * 2 - 1).float() * (torch.rand((grid_h - 1) * grid_w) * (5 - 0.05) + 0.05)).view(grid_h-1, grid_w)\n",
    "    if random_seed is not None:\n",
    "        torch.set_rng_state(rng_state)\n",
    "    return horizontal_interaction, vertical_interaction\n",
    "\n",
    "class Ising(object):\n",
    "    \"\"\"\n",
    "    Ising Sparsification Problem with the simplest graph\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lamda, random_seed_pair=(None, None)):\n",
    "        self.lamda = lamda\n",
    "        self.n_vertices = np.array([2] * ISING_N_EDGES)\n",
    "        self.suggested_init = torch.empty(0).long()\n",
    "        self.suggested_init = torch.cat([self.suggested_init, sample_init_points(self.n_vertices, 20 - self.suggested_init.size(0), random_seed_pair[1]).long()], dim=0)\n",
    "        self.adjacency_mat = []\n",
    "        self.fourier_freq = []\n",
    "        self.fourier_basis = []\n",
    "        self.random_seed_info = 'R'.join([str(random_seed_pair[i]).zfill(4) if random_seed_pair[i] is not None else 'None' for i in range(2)])\n",
    "        for i in range(len(self.n_vertices)):\n",
    "            n_v = self.n_vertices[i]\n",
    "            adjmat = torch.diag(torch.ones(n_v - 1), -1) + torch.diag(torch.ones(n_v - 1), 1)\n",
    "            self.adjacency_mat.append(adjmat)\n",
    "            laplacian = torch.diag(torch.sum(adjmat, dim=0)) - adjmat\n",
    "            eigval, eigvec = torch.symeig(laplacian, eigenvectors=True)\n",
    "            self.fourier_freq.append(eigval)\n",
    "            self.fourier_basis.append(eigvec)\n",
    "        interaction = generate_ising_interaction(ISING_GRID_H, ISING_GRID_W, random_seed_pair[0])\n",
    "        self.interaction = interaction[0].numpy(), interaction[1].numpy()\n",
    "        self.covariance, self.partition_original = spin_covariance(self.interaction, (ISING_GRID_H, ISING_GRID_W))\n",
    "\n",
    "    def evaluate(self, x):\n",
    "        if x.dim() == 1:\n",
    "            x = x.unsqueeze(0)\n",
    "        assert x.size(1) == len(self.n_vertices)\n",
    "        return torch.cat([self._evaluate_single(x[i]) for i in range(x.size(0))], dim=0)\n",
    "\n",
    "    def _evaluate_single(self, x):\n",
    "        assert x.dim() == 1\n",
    "        x_h, x_v = _bocs_consistency_mapping(x.numpy())\n",
    "        interaction_sparsified = x_h * self.interaction[0], x_v * self.interaction[1]\n",
    "        log_partition_sparsified = log_partition(interaction_sparsified, (ISING_GRID_H, ISING_GRID_W))\n",
    "        evaluation = ising_dense(interaction_sparsified=interaction_sparsified, interaction_original=self.interaction,\n",
    "                                 covariance=self.covariance, log_partition_sparsified=log_partition_sparsified,\n",
    "                                 log_partition_original=np.log(self.partition_original))\n",
    "        evaluation += self.lamda * float(torch.sum(x))\n",
    "        return evaluation * x.new_ones((1,)).float()\n",
    "\n",
    "\n",
    "def _contamination(x, cost, init_Z, lambdas, gammas, U, epsilon):\n",
    "    assert x.size == CONTAMINATION_N_STAGES\n",
    "\n",
    "    rho = 1.0\n",
    "    n_simulations = 100\n",
    "\n",
    "    Z = np.zeros((x.size, n_simulations))\n",
    "    Z[0] = lambdas[0] * (1.0 - x[0]) * (1.0 - init_Z) + (1.0 - gammas[0] * x[0]) * init_Z\n",
    "    for i in range(1, CONTAMINATION_N_STAGES):\n",
    "        Z[i] = lambdas[i] * (1.0 - x[i]) * (1.0 - Z[i - 1]) + (1.0 - gammas[i] * x[i]) * Z[i - 1]\n",
    "\n",
    "    below_threshold = Z < U\n",
    "    constraints = np.mean(below_threshold, axis=1) - (1.0 - epsilon)\n",
    "\n",
    "    return np.sum(x * cost - rho * constraints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "464c4174",
   "metadata": {},
   "outputs": [],
   "source": [
    "ii = Ising(lamda=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1f428ab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ii.adjacency_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "115c93f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_evaluate_single',\n",
       " 'adjacency_mat',\n",
       " 'covariance',\n",
       " 'evaluate',\n",
       " 'fourier_basis',\n",
       " 'fourier_freq',\n",
       " 'interaction',\n",
       " 'lamda',\n",
       " 'n_vertices',\n",
       " 'partition_original',\n",
       " 'random_seed_info',\n",
       " 'suggested_init']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(ii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03949a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([21, 21])\n",
      "torch.Size([21, 21])\n"
     ]
    }
   ],
   "source": [
    "b = Branin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "609825aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10.5000, 10.5000],\n",
       "        [ 0.0000, 12.0000]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.suggested_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a078b7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
