{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 671,
   "id": "c19ecd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Tuple, List, Iterable, Callable\n",
    "\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import halfcauchy, invgamma\n",
    "import cvxpy as cp\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "from scipy.stats import multivariate_normal\n",
    "from numpy.random import default_rng\n",
    "from itertools import chain, combinations, product\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ff741e",
   "metadata": {},
   "source": [
    "## utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "id": "b288da69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def powerset(x:Iterable):\n",
    "    '''\n",
    "    Powerset (set of subsets) for a given iterable x incl. ∅\n",
    "    '''\n",
    "    s = list(x)\n",
    "    powerSet = chain.from_iterable(combinations(s, r) for r in range(len(s)+1))\n",
    "    \n",
    "    return list(powerSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2659a4",
   "metadata": {},
   "source": [
    "## models.py\n",
    "\n",
    "`Oracle`: \n",
    "- **TODO** Proper management of access counts (`N_total` ?)\n",
    "- clarify roles of `N_total` and `N_current`\n",
    "- consider how to incorporate `seedVal` to  function call `f`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "id": "81261657",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxQueriesExceeded(Exception):\n",
    "    def __init__(self, message):            \n",
    "        # Call the base class constructor with the parameters it needs\n",
    "        super().__init__(message)\n",
    "        \n",
    "        \n",
    "\n",
    "class Oracle:\n",
    "    def __init__(self, fun, order:int=2, sigma_2:float=0.0, N_total:int=1000, seed:int=0):\n",
    "        '''\n",
    "        Noise terms are sampled at initialization to s.t. sampled function values f(x_i) are \n",
    "        reporducible regardless how the oracle is sampled.\n",
    "        '''\n",
    "        assert isinstance(fun, Callable), \"Input `f` must be a callable function.\"\n",
    "        assert isinstance(order, int) and 1<=order<6, \"Maximum `order` of interaction terms should be 5.\"\n",
    "        assert isinstance(sigma_2, float) and sigma_2>=0, \"Noise variance parameter `sigma_2` must be a non-negative float.\"\n",
    "        assert isinstance(seed, int), \"Seed for random draws `seed` must be an integer.\"\n",
    "        assert isinstance(N_total, int), \"Maximum number of queries `N_total` must be an positive integer.\"\n",
    "        \n",
    "        self.fun = fun\n",
    "        self.order = order\n",
    "        self.sigma_2 = sigma_2\n",
    "        self.seed = seed\n",
    "        self.N_total = N_total\n",
    "        self.N_current = 0\n",
    "        \n",
    "        if(self.sigma_2 > 0):\n",
    "            np.random.seed(self.seed)\n",
    "            self.eps = np.random.normal(loc=0, scale=np.sqrt(self.sigma_2), size=self.N_total)\n",
    "        else:\n",
    "            self.eps = np.zeros(self.N_total)\n",
    "    \n",
    "    def __expandX__(self, x:np.array) -> np.array:\n",
    "        '''\n",
    "        Expand a binary input vector `x` from the original input format with indices {1,...,d} to\n",
    "        {0} (intercept), {1,...,d} (1st-order coefs), {... (d over 2) ... } (2nd-order effects)\n",
    "        '''\n",
    "        \n",
    "        # \n",
    "        matrixInput = len(x.shape)==2 and len(x)>1\n",
    "        \n",
    "        # transform\n",
    "        if(matrixInput):\n",
    "            highOrdMats = [np.ones(len(x)).reshape(-1,1), x]\n",
    "            # o-th order : CORRECT\n",
    "            for o in range(2, self.order+1):\n",
    "                highOrdMats.append(np.stack([np.prod([x[:,pair[i]] for i in range(o)], axis=0) for pair in powerset(range(x.shape[1])) if len(pair)==o], axis=1))\n",
    "\n",
    "            #print('highOrdMats : ', highOrdMats)\n",
    "            x = np.concatenate(highOrdMats, axis=1)\n",
    "        else:\n",
    "            # o-th order : CORRECT\n",
    "            x = np.array(x, dtype=np.float64)\n",
    "            highOrdMats = [x]\n",
    "            for o in range(2, self.order+1):\n",
    "                highOrdMats.append(np.array([np.prod([x[pair[i]] for i in range(o)]) for pair in powerset(range(len(x))) if len(pair)==o]))\n",
    "\n",
    "            #print(highOrdMats)\n",
    "            x = np.concatenate(([np.array([1])] + highOrdMats), axis=0)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def f(self, x:np.array):\n",
    "        '''\n",
    "        Returns (noisy) function value f(x). \n",
    "        If `noiseFlag` is set to True, f(x) + eps with eps ~ N(f(x), sigma_2) is returned.\n",
    "        '''\n",
    "    \n",
    "        # expand raw input\n",
    "        #x = self.__expandX__(x)\n",
    "        \n",
    "        matInputFlag = len(x.shape)==2 and len(x)>1\n",
    "        \n",
    "        # check if within remaining query budget\n",
    "        n_req = len(x) if matInputFlag else 1\n",
    "        \n",
    "        if(self.N_current + n_req > self.N_total):\n",
    "            raise MaxQueriesExceeded(f\"Maximum number of queries `N_total`={self.N_total} would be exceeded with these `n_req`={n_req} additional requests given that `N_current=`{self.N_current}.\")\n",
    "        \n",
    "        # compute (noisy) function value\n",
    "        if(matInputFlag):\n",
    "            fx =  self.fun(x) + self.eps[self.N_current:self.N_current+len(x)] if (self.sigma_2 > 0) else np.zeros(len(x))\n",
    "        else:\n",
    "            fx =  self.fun(x) + (float(self.eps[self.N_current]) if (self.sigma_2 > 0) else 0)\n",
    "        \n",
    "        # update budget\n",
    "        self.N_current += n_req\n",
    "        \n",
    "        return fx\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a90ffb",
   "metadata": {},
   "source": [
    "## `sklearn` as a reference\n",
    "\n",
    "How & what errors/exceptions are being triggered\n",
    "\n",
    "`lm.fit(X=4,y=4)`\n",
    "\n",
    "```\n",
    "ValueError: Expected 2D array, got scalar array instead: array=4.\n",
    "Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample\n",
    "```\n",
    "\n",
    "`X is singul`\n",
    "No \n",
    "```\n",
    "LinAlgError: Singular matrix\n",
    "``` \n",
    "is being triggered. `LinearModel` has numerical routines to circumvent this.\n",
    "\n",
    "Setting a value, e.g. `lm.coef_+5`\n",
    "No exception is being triggered (of course)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "id": "39cf39c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-18 {color: black;background-color: white;}#sk-container-id-18 pre{padding: 0;}#sk-container-id-18 div.sk-toggleable {background-color: white;}#sk-container-id-18 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-18 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-18 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-18 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-18 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-18 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-18 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-18 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-18 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-18 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-18 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-18 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-18 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-18 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-18 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-18 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-18 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-18 div.sk-item {position: relative;z-index: 1;}#sk-container-id-18 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-18 div.sk-item::before, #sk-container-id-18 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-18 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-18 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-18 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-18 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-18 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-18 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-18 div.sk-label-container {text-align: center;}#sk-container-id-18 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-18 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-18\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-18\" type=\"checkbox\" checked><label for=\"sk-estimator-id-18\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 674,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# init object\n",
    "lm = LinearRegression()\n",
    "\n",
    "# data\n",
    "n, k = 100, 5\n",
    "np.random.seed(3457)\n",
    "X = np.random.normal(loc=1,scale=1.5,size=n*k).reshape(n,-1)\n",
    "y = (2.5*X).sum(axis=1) + np.random.normal(loc=0,scale=0.5,size=n).flatten() + 5\n",
    "\n",
    "# fit\n",
    "lm.fit(X,y) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6ba7bb",
   "metadata": {},
   "source": [
    "## Test `Oracle`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "id": "6dc0d018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 3 4]\n",
      " [1 1 2]\n",
      " [3 3 4]]\n"
     ]
    }
   ],
   "source": [
    "o1 = Oracle(fun=lambda x: np.sum(x, axis=1), sigma_2=0.0, N_total=2000, seed=777, order=3)\n",
    "\n",
    "X = np.array([[0, 3, 4], [1, 1, 2], [3, 3, 4]])\n",
    "#X = np.random.normal(loc=0, scale=2, size=10 * 3).reshape(-1, 3)\n",
    "\n",
    "# show\n",
    "print(X)\n",
    "\n",
    "y = o1.fun(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 918,
   "id": "f204659e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1,   2,   3,   4,   5,   6,   7, 777, 666])"
      ]
     },
     "execution_count": 918,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate((np.array([1,2,3,4,5,6,7]), np.array([777, 666])), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 994,
   "id": "a1d45cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseBayesReg:\n",
    "    def __init__(self, N_total:int, order:int=2, seed:int=0, burnin:int=0, thinning:int=1, d_MAX:int=10):\n",
    "\n",
    "        assert isinstance(seed, int), \"`seed` must be an integer.\"\n",
    "        assert isinstance(burnin, int) and burnin>=0, \"`burnin` must be a non-negative integer.\"\n",
    "        assert isinstance(thinning, int) and thinning>=1, \"`thinning` must be an positive integer.\"\n",
    "        \n",
    "        # - assignment\n",
    "        self.N_total = N_total\n",
    "        self.order = order\n",
    "        self.seed = seed\n",
    "        self.burnin = burnin\n",
    "        self.thinning = thinning\n",
    "        self.d_MAX = d_MAX\n",
    "        \n",
    "    def setXy(self, X:np.array, y:np.array) -> None:\n",
    "        '''\n",
    "        Setup of design matrix X (standardized, incl. leading 1-column and quadratic terms) and target vector y (transalted to E[y]=0)\n",
    "        '''\n",
    "        assert sum(X[:,0])!=len(X), \"Provide the design matrix X without adding a leading 1-column (for intercept)\"\n",
    "        \n",
    "        self.d = X.shape[1]\n",
    "        self.p = sum([math.comb(self.d, k) for k in range(0,self.order+1)])\n",
    "        \n",
    "        assert isinstance(self.d, int) and 0<self.d<=self.d_MAX, f\"The inferred dimension `d`={self.d} should be smaller than {self.d_MAX}\"\n",
    "        \n",
    "        X = self.__expandX__(X)         # arbitrary interaction effects of order k\n",
    "        X = self.__standardizeX__(X)\n",
    "        X = self.__interceptColumnX__(X)\n",
    "        self.X = X\n",
    "        \n",
    "        #y = self.__translateY__(y)  # TEST\n",
    "        self.y = y\n",
    "        \n",
    "    def setX(self, X:np.array) -> None:\n",
    "        '''\n",
    "        Setup of design matrix X(_new) for prediction only.\n",
    "        '''\n",
    "        assert sum(X[:,0])!=len(X), \"Provide the design matrix X without adding a leading 1-column (for intercept)\"\n",
    "        assert X.shape[1]==self.d, f\"Input matrix `X` has {X.shape[1]} columns but {self.d} was expected. Provide input with original dimension `d`.\"\n",
    "\n",
    "        X = self.__expandX__(X)\n",
    "        X = self.__standardizeX__(X, trainMode=False)\n",
    "        X = self.__interceptColumnX__(X)\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    def __expandX__(self, X:np.array) -> None:\n",
    "        '''\n",
    "        Given a (binary) design matrix X, it appends pairwise products of columns and appends to the design matrix X;\n",
    "        *excluding* the leading intercept column of 1's\n",
    "        '''\n",
    "        \n",
    "        assert self.order<=X.shape[1], f\"`Order` of interaction terms can be at most number of columns of design matrix `X`. Lower the `order` to `X.shape[1]`={X.shape[1]}.\"\n",
    "        if(self.order==X.shape[1]):\n",
    "            raise np.linalg.LinAlgError(\"Setting `order` equal to the number of columns of the design matrix will cause parameters to be unidentifiable.\")\n",
    "        \n",
    "        # append to arbitrary terms\n",
    "        xList = [X]\n",
    "        # o-th order : CORRECT\n",
    "        for k in range(2, self.order+1):\n",
    "            xList.append(np.stack([np.prod([X[:,pair[i]] for i in range(k)], axis=0) for pair in powerset(range(X.shape[1])) if len(pair)==k], axis=1))\n",
    "\n",
    "        #print('highOrdMats : ', highOrdMats)\n",
    "        X = np.concatenate(xList, axis=1)\n",
    "        \n",
    "        return X\n",
    "\n",
    "    def __standardizeX__(self, X:np.array, trainMode:bool=True) -> None:\n",
    "        '''\n",
    "        Standardizes (translates & rescales) the columns of the design matrix (input matrix) \n",
    "        '''\n",
    "\n",
    "        assert X.shape[1]==self.p-1, \"The given design matrix includes a leading 1-column; unclear if it is a legitimiate (coincidental) feature or leading 1 column was already incldued\"\n",
    "\n",
    "        if(trainMode):\n",
    "            X_mu, X_sigma = X.mean(axis=0), X.std(axis=0)\n",
    "            self.X_mu = X_mu\n",
    "            self.X_sigma = X_sigma  #np.sqrt(len(X)) * X_sigma  # corrected for sqrt(n)\n",
    "        else:\n",
    "            assert (self.X_mu is not None) and (self.X_sigma is not None), \"`X_mu` and `X_sigma` must be pre-computed.\"\n",
    "        \n",
    "        X = (X - self.X_mu) / self.X_sigma\n",
    "        \n",
    "        return X\n",
    "        \n",
    "    def __interceptColumnX__(self, X:np.array) -> np.array:\n",
    "        '''\n",
    "        Adds a leading vector of 1s to the binary matrix X (of 1st and 2nd order interactions)\n",
    "        '''\n",
    "        \n",
    "        X = np.concatenate((np.ones_like(X[:,0]).reshape(-1,1), X), axis=1)\n",
    "\n",
    "        assert X.shape[1]==self.p, \"Inconsistent number of columns after adding leading 1-col\"\n",
    "        \n",
    "        return X\n",
    "\n",
    "    def __translateY__(self, y:np.array, trainMode:bool=True) -> None:\n",
    "        '''\n",
    "        Translation of the target vector y such that priori condition E[y]=0 is satisfied.\n",
    "        (No rescaling to unit variance is applied, though.)\n",
    "        '''\n",
    "        X = self.X\n",
    "\n",
    "        assert len(X) == len(y), \"Length of target vector y does not coincide with design matrix X\"\n",
    "\n",
    "        # Standardize y's\n",
    "        if(trainMode):\n",
    "            self.y_mu = np.mean(y)\n",
    "        else:\n",
    "            assert self.y_mu, \"`y_mu` must be computed.\"\n",
    "        \n",
    "        # translation\n",
    "        y = y - self.y_mu\n",
    "        \n",
    "        return y\n",
    "        \n",
    "    def add(self, X:np.array, y:float, fitFlag:bool=True) -> None:\n",
    "        '''\n",
    "        Appends new datapoint to X,y\n",
    "        '''\n",
    "        \n",
    "        # YYY\n",
    "        assert isinstance(X, np.ndarray), \"New data `X` must be provided as a numpy array.\"\n",
    "        \n",
    "        # convert obs. vector (n=1)\n",
    "        if len(X.shape)==1:\n",
    "            X = X.reshape(1, len(X))\n",
    "            if(isinstance(y, float) or isinstance(y, int)):\n",
    "                y = np.array([y])\n",
    "        \n",
    "        assert X.shape[1]==self.d, \"New data input should have column dim. `d`.\"\n",
    "        assert len(X)==len(y), \"Lengths of new data `X` and `y` do not coincide.\"\n",
    "        \n",
    "        # New\n",
    "        #X = self.__expandX__(X)\n",
    "        #X = self.__standardizeX__(X, trainMode=False)\n",
    "        #X = self.__interceptColumnX__(X)\n",
    "        \n",
    "        # Newer\n",
    "        X = self.setX(X)\n",
    "        \n",
    "        assert self.X.shape[1]==X.shape[1], \"Number of columns of new data must coincide with design matrix `X`.\"\n",
    "        \n",
    "        # append to data\n",
    "        self.X = np.concatenate((np.array(self.X), X), axis=0)\n",
    "        self.y = np.concatenate((np.array(self.y), y), axis=0)\n",
    "        \n",
    "        # re-fit\n",
    "        if(fitFlag):\n",
    "            self.__fit__()\n",
    "            \n",
    "        return None\n",
    "    \n",
    "    def __mvg__(self, Phi, alpha, D):\n",
    "        '''\n",
    "        Sample multivariate Gaussian (independent of d) from NumPy\n",
    "        Not used Rue (2001) or et. al. (2015) approaches on fast sampling mvg\n",
    "        N(mean = S@Phi.T@y, cov = inv(Phi'Phi + inv(D))\n",
    "        '''\n",
    "        #assert len(Phi.shape)==2 and Phi.shape[0]==Phi.shape[1], \"`Phi` must be a quadratic matrix.\"\n",
    "        assert len(D.shape)==2 and D.shape[0]==D.shape[1], \"`D` must be a quadratic matrix.\"\n",
    "        \n",
    "        S = np.linalg.inv(Phi.T @ Phi + np.linalg.inv(D))\n",
    "        x = np.random.multivariate_normal(mean=((S @ Phi.T) @ y), cov=S, size=1)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def sampleStandardizedAlpha(self, ) -> np.array:\n",
    "        '''\n",
    "        Samples posterior  ~ P( |X_tilde,y) from (most current) posterior distribution \n",
    "        parametrized by the standardized design matrix X_tilde.\n",
    "        '''\n",
    "        \n",
    "        assert (self.alpha_mu is not None) and (self.alpha_cov is not None), \"Posterior mean and covariance not available yet as the model been computed yet. Run `.fit(X,y)`.\"\n",
    "        \n",
    "        alpha_post = np.random.multivariate_normal(mean = self.alpha_mu,\n",
    "                                                   cov  = self.alpha_cov,\n",
    "                                                   size = 1).reshape(-1)\n",
    "        \n",
    "        return alpha_post\n",
    "\n",
    "    def sampleAlpha(self, ) -> np.array:\n",
    "        '''\n",
    "        Samples posterior  ~ P( |X,y) from (most current) posterior distribution \n",
    "        parametrized by the design matrix X.\n",
    "        '''\n",
    "        \n",
    "        alpha_post = np.array(self.sampleStandardizedAlpha())\n",
    "        \n",
    "        # correct w.r.t. X_tilde = (X - X_mu) / X_sigma\n",
    "        alpha_post[1:] /= self.X_sigma\n",
    "        alpha_post[0]  -= self.X_mu @ alpha_post[1:] \n",
    "        \n",
    "        return alpha_post\n",
    "    \n",
    "    def getMeanStandardizedAlpha(self, ) -> np.array:\n",
    "        '''\n",
    "        Posterior mean of alpha based on the row-wise standardized design matrix X_tilde\n",
    "        '''\n",
    "        \n",
    "        return np.array(self.alpha_mu)\n",
    "    \n",
    "    def getMeanAlpha(self, ) -> np.array:\n",
    "        '''\n",
    "        Posterior mean of alpha based on the (actual) design matrix X.\n",
    "        '''\n",
    "        alpha_mu = np.array(self.getMeanStandardizedAlpha())\n",
    "        \n",
    "        # correct w.r.t. X_tilde = (X - X_mu) / X_sigma\n",
    "        alpha_mu[1:] /= self.X_sigma\n",
    "        alpha_mu[0]  -= self.X_mu @ alpha_mu[1:]  \n",
    "        \n",
    "        return alpha_mu\n",
    "    \n",
    "    # sbr.X_mu @ sbr.getMeanAlpha()[1:]\n",
    "    \n",
    "    def __fit__(self) -> None:\n",
    "        '''\n",
    "        Core of fitting procedure (on self.X, self.y)\n",
    "        '''\n",
    "        \n",
    "        # TEST\n",
    "        print(f'self.p : {self.p}')\n",
    "        \n",
    "        # D0\n",
    "        self.n = len(self.X)\n",
    "        \n",
    "        # setup values\n",
    "        alphas_out = np.zeros((self.p, 1))\n",
    "        s2_out     = np.zeros((1, 1))\n",
    "        t2_out     = np.zeros((1, 1))\n",
    "        l2_out     = np.zeros((self.p, 1))\n",
    "\n",
    "        # sample priors\n",
    "        betas   = halfcauchy.rvs(size=self.p) \n",
    "        tau_2   = halfcauchy.rvs(size=1)                            \n",
    "        nu      = np.ones(self.p) # ?\n",
    " \n",
    "        sigma_2, xi = 1.0, 1.0\n",
    "        \n",
    "        # Gibbs sampler\n",
    "        for k in range(self.N_total):\n",
    "            sigma = np.sqrt(sigma_2)\n",
    "\n",
    "            # alphas\n",
    "            # - Sigma_star\n",
    "            Sigma_star = tau_2 * np.diag(betas**2) # Sigma_star\n",
    "            Sigma_star_inv = np.linalg.inv(Sigma_star)\n",
    "            \n",
    "            # - A\n",
    "            A     = (self.X.T @ self.X) + Sigma_star_inv\n",
    "            A_inv = np.linalg.inv(A)\n",
    "            \n",
    "            # - update posterior mean, cov\n",
    "            self.alpha_mu  = A_inv @ self.X.T @ self.y\n",
    "            self.alpha_cov = sigma_2 * A_inv\n",
    "            \n",
    "            # - alpha\n",
    "            alphas = self.sampleStandardizedAlpha()\n",
    "            \n",
    "            # - sigma_2\n",
    "            sigma_2 = invgamma.rvs(0.5*(self.n+self.p), scale=0.5*(np.linalg.norm((self.y - self.X @ alphas), 2)**2 + (alphas.T @ Sigma_star_inv @ alphas)))\n",
    "\n",
    "            # - betas\n",
    "            betas_2 = invgamma.rvs(np.ones(self.p), scale=(1. / nu) + (alphas**2)/(2. * tau_2 * sigma_2))\n",
    "            betas = np.sqrt(betas_2)\n",
    "\n",
    "            # - tau_2\n",
    "            tau_2 = invgamma.rvs(0.5*(self.p+1), scale=1.0 / xi + (1. / (2. * sigma_2)) * sum(alphas**2 / betas**2), size=1)\n",
    "\n",
    "            # - nu\n",
    "            nu = invgamma.rvs(np.ones(self.p), scale=1.0 + 1. / betas_2, size=self.p)\n",
    "\n",
    "            # - xi\n",
    "            xi = invgamma.rvs(1.0, scale=1.0 + 1. / tau_2, size=1)\n",
    "            \n",
    "            # store samples\n",
    "            if k > self.burnin:\n",
    "                # - append\n",
    "                if(k%self.thinning==0):\n",
    "                    alphas_out = np.append(arr=alphas_out, values=alphas.reshape(-1,1), axis=1)\n",
    "                    s2_out = np.append(s2_out, sigma_2)\n",
    "                    t2_out = np.append(t2_out, tau_2)\n",
    "                    l2_out = np.append(arr=l2_out, values=betas.reshape(-1,1), axis=1)\n",
    "\n",
    "        # Clip 1st value\n",
    "        self.alphas = alphas_out[:,1:]\n",
    "        self.s2 = s2_out[1:]\n",
    "        self.t2 = t2_out[1:]\n",
    "        self.l2 = l2_out[1:]\n",
    "        \n",
    "    def getStandardizedAlphas(self) -> np.array:\n",
    "        '''\n",
    "        Returns current array of alpha posterior samples (based on the standardized design matrix X_tilde)\n",
    "        '''\n",
    "        return self.alphas\n",
    "    \n",
    "    def fit(self, X:np.array, y:np.array) -> None:\n",
    "        '''\n",
    "        Fitting the (initial) model on the data D0={X0,y0}\n",
    "        '''\n",
    "        assert len(X.shape)==2 and len(y.shape)==1, \"Design matrix X and target vector y.\"\n",
    "        assert X.shape[0]==len(y), f\"Dimension of design matrix X and target vector y do not coincide: X.shape[1]={X.shape[1]}!={len(y)}=len(y)\"\n",
    "        assert len(X) < self.N_total, f\"Implied `N_init`=len(X)={len(X)} exceeds `N_total`={self.N_total}.\"\n",
    "        \n",
    "        # setup\n",
    "        self.setXy(X, y)\n",
    "        \n",
    "        # fitting\n",
    "        self.__fit__()\n",
    "        \n",
    "    def predict(self, X:np.array, mode:str='mean') -> np.array:\n",
    "        '''\n",
    "        Obtain prediction\n",
    "        '''\n",
    "        assert mode in ['mean', 'post'], \"`predict`ion from Bayesian sparse regression either by `mean` (MLE estimator of alpha) or randomly sampled from `post`erior.\"\n",
    "        assert X.shape[1] == self.d, f\"Format of input matrix wrong. Does not have `d`={self.d} columns but `X.shape[1]`={X.shape[1]}\"\n",
    "        \n",
    "        # TraFo data\n",
    "        X = self.setX(X)\n",
    "        \n",
    "        # apply model\n",
    "        alpha_hat = self.getMeanStandardizedAlpha() if mode=='mean' else self.sampleStandardizedAlpha()\n",
    "        \n",
    "        # dot prod\n",
    "        y_hat = X @ alpha_hat\n",
    "        \n",
    "        # revsert-translation\n",
    "        #y_hat = y_hat + self.y_mu # TEST\n",
    "        \n",
    "        \n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bd62b0",
   "metadata": {},
   "source": [
    "## Test `SparseBayesReg`\n",
    "\n",
    "- check `fit` ✅\n",
    "    - model 1 : $f(x) = 5 + \\sum_{i=1}^{d} 2 \\cdot x_{i}$\n",
    "    - model 2 : $x^{2}$ ... i.e. $f(x) = 10 + \\sum_{i=1}^{d} 2 \\cdot x_{i}$\n",
    "    \n",
    "- works for various regression models/orders ✅\n",
    "    \n",
    "- check `predict` ✅\n",
    "\n",
    "### Remains to test\n",
    "- Add a new observation (row) ✅\n",
    "- Check if refitting works  ✅\n",
    "    - additionally, allow to decide if adding data triggers re-fit everytime (it shouldn't).  ✅\n",
    "- optimize how many initial Gibbs sampling runs have to be performed (vs. new gibs runs during re-fitting) `???`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 998,
   "id": "24f539bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.p : 7\n"
     ]
    }
   ],
   "source": [
    "# set data\n",
    "sbr = SparseBayesReg(N_total=5000, order=2, burnin=100, thinning=2)\n",
    "\n",
    "# fit model\n",
    "sbr.fit(X, y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fe53eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SDP:\n",
    "    def __init__(self, alpha:np.array, lambd:float=0.1, pen_ord:int=2, mode:str='min', d_MAX:int=20) -> List[np.array]:\n",
    "        \n",
    "        assert isinstance(mode, str), \"Input `mode` must be a either `min` or `max`.\"\n",
    "        assert mode in ['min', 'max'], f\"Input `mode` is str. In addition, it must be a str either `min` or `max` but `{mode}` was provided.\"\n",
    "        assert isinstance(lambd, float) and lambd >=0, \"lambda (regularization parameter) must be non-negative scalar.\"\n",
    "        assert pen_ord in [1, 2] \"Penalty norm order `pen_ord` must be either 1 or 2.\"\n",
    "        \n",
    "        self.mode = mode\n",
    "        self.lambd = lambd\n",
    "        self.pen_ord = pen_ord\n",
    "        self.alpha = alpha\n",
    "        self.d_MAX = d_MAX\n",
    "        \n",
    "        # infer d(imension)\n",
    "        self.p = len(self.alpha)\n",
    "        dDict = {1+dLoc+math.comb(dLoc,2) : dLoc for dLoc in range(1,self.d_MAX+1)}\n",
    "        if(self.p in dDict.keys()):\n",
    "            self.d = dDict[self.p]\n",
    "        else:\n",
    "            assert False, f'Length of `alpha` is not a 1+d+binom(d,2) for any 1,2,...,{self.d_MAX}'\n",
    "        assert isinstance(self.d, int), \"Dimension `d` must be non-negative integer.\"\n",
    "        \n",
    "        # extract 1st/2nd order terms\n",
    "        b = self.alpha[1:1 + self.d]  # 1st\n",
    "        a = self.alpha[1 + self.d:]   # 2nd\n",
    "\n",
    "        # get indices for quadratic terms\n",
    "        idx_prod = np.array(list(combinations(np.arange(self.d), 2)))\n",
    "        d_idx = idx_prod.shape[0]\n",
    "\n",
    "        # check number of coefficients\n",
    "        if len(a)!=d_idx:\n",
    "            assert False, 'Number of Coefficients does not match indices!'\n",
    "\n",
    "        # xAx-term\n",
    "        A = np.zeros((self.d, self.d))\n",
    "        for i in range(d_idx):\n",
    "            A[idx_prod[i,0], idx_prod[i,1]] = 0.5 * a[i]\n",
    "        A += A.T\n",
    "\n",
    "        # bx-term\n",
    "        bt = 0.5 * (b + A @ np.ones(self.d)).reshape((-1, 1))\n",
    "        bt = bt.reshape((self.d, 1))\n",
    "        At = np.vstack((np.append(0.25*A, 0.25*bt, axis=1), np.append(bt.T, 2.)))\n",
    "        \n",
    "        self.A  = A\n",
    "        self.b  = b\n",
    "        self.At = At\n",
    "        self.bt = bt\n",
    "        \n",
    "    def run(self) -> np.array:\n",
    "        '''\n",
    "        Runs the BQP-relaxation, SDP-optimization, and extracts candidate x via geometric rounding.\n",
    "        '''\n",
    "        self.solve()\n",
    "        self.decompose()\n",
    "        return self.geometricRounding()\n",
    "        \n",
    "        \n",
    "    def solve(self, ) -> None:\n",
    "        '''Actual solver of the semi-definite programming solver.'''\n",
    "        \n",
    "        # SDP relaxation\n",
    "        Xvar = cp.Variable((self.d+1, self.d+1), PSD=True)\n",
    "        \n",
    "        # - objective function\n",
    "        if(self.mode=='min'):\n",
    "            f0 = cp.Minimize(cp.trace((self.At @ Xvar)))\n",
    "        else:\n",
    "            f0 = cp.Maximize(cp.trace((self.At @ Xvar)))\n",
    "        \n",
    "        # - constraints\n",
    "        constraints = [cp.diag(Xvar) == np.ones(self.d+1)]\n",
    "        prob = cp.Problem(f0, constraints)\n",
    "        prob.solve()\n",
    "        \n",
    "        self.Xvar = Xvar\n",
    "        \n",
    "    def decompose(self) -> None:\n",
    "        '''\n",
    "        Wrapper for stable Cholesky decomposition\n",
    "        '''\n",
    "        self.L = self.__stableCholesky__(eTol=1E-12)\n",
    "    \n",
    "    def __stableCholesky__(self, eTol:float=1E-10) -> np.array:\n",
    "        '''\n",
    "        Performs numerically stable Cholesky decomposition (by adding regularity to the matrix until PSD). \n",
    "        '''\n",
    "        try:\n",
    "            return np.linalg.cholesky(self.Xvar.value + eTol*np.eye(self.Xvar.value.shape[0]))\n",
    "        except Exception as e:\n",
    "            if(isinstance(e, np.linalg.LinAlgError)):\n",
    "                return self.__stableCholesky__(10*eTol)\n",
    "            else:\n",
    "                pass\n",
    "    \n",
    "    def geometricRounding(self, k_rounds:int=100) -> np.array:\n",
    "        '''\n",
    "        Random geometric round and conversion to original space\n",
    "        - k_rounds: number of iterations\n",
    "        '''\n",
    "        x_cand  = np.zeros((self.d, k_rounds))\n",
    "        f_star  = np.zeros(k_rounds)\n",
    "\n",
    "        for j in range(k_rounds):\n",
    "            # rnd cutting plane vector (U on Sn) \n",
    "            r = np.random.randn(self.d+1)\n",
    "            r /= np.linalg.norm(r, ord=2)\n",
    "            \n",
    "            # rnd hyperplane\n",
    "            y_star = np.sign(self.L.T @ r)\n",
    "\n",
    "            # convert solution to original domain and assign to output vector\n",
    "            x_cand[:,j] = 0.5 * (1.0 + y_star[:self.d])\n",
    "            f_star[j] = (x_cand[:,j].T @ self.A @ x_cand[:,j]) + (self.b @  x_cand[:,j])\n",
    "\n",
    "            # Find optimal rounded solution\n",
    "            if(self.mode=='min'):\n",
    "                f_argopt = np.argmin(f_star)\n",
    "            else:\n",
    "                f_argopt = np.argmax(f_star)\n",
    "            x_0      = x_cand[:,f_argopt]\n",
    "            f_0      = f_star[f_argopt]\n",
    "\n",
    "        return (x_0, f_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "d28e03d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BOCS:\n",
    "    variantList = ['SDP', 'SA']\n",
    "    optModes = ['min', 'max']\n",
    "    \n",
    "    def __init__(self, variant:str='SDP', oracle:Oracle=None, N:int=1, B:int=0, T:int=1, seed:int=0,\n",
    "                 lambd:float=0.5, pen_ord:int=2, mode:str='min'):\n",
    "        \n",
    "        assert variant in self.variantList, f\"AFO `variant` must be in: {', '.join(variantList)}\"\n",
    "        \n",
    "        self.variant = variant\n",
    "        self.N = N\n",
    "        self.seed = seed\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "        self.lambd = lambd\n",
    "        self.pen_ord = pen_ord\n",
    "        self.mode = mode\n",
    "        self.oracle = oracle\n",
    "        \n",
    "        # init Sparse Bayesian Regression\n",
    "        self.BayReg = SparseBayesReg(N_total=self.N, burnin=self.B, thinning=self.T, seed=self.seed)\n",
    "        \n",
    "    def fit(self, X:np.array, y:np.array) -> None:\n",
    "        '''\n",
    "        Delegate fit to bayesian regression model\n",
    "        '''\n",
    "        self.BayReg.setXy(X,y)\n",
    "        self.BayReg.fit(X,y)\n",
    "        \n",
    "    def update(self,):\n",
    "        '''\n",
    "        Sample alpha from Bayesian regression, solve SDP, return cancidate x & (noisy) oracle function value y\n",
    "        '''\n",
    "        alpha_t = self.BayReg.sampleAlpha()\n",
    "        \n",
    "        # SDP update\n",
    "        if(self.variant=='SDP'):\n",
    "            self.sdp = SDP(alpha=alpha_t, lambd=self.lambd, pen_ord=self.pen_ord, mode=self.mode)\n",
    "            x_new, y_new_hat = self.sdp.run()\n",
    "            y_new = self.oracle.f(x_new)\n",
    "        elif(self.variant=='SA'):\n",
    "            pass\n",
    "        else:\n",
    "            assert False, \"Not implemented\"\n",
    "        \n",
    "        # update model\n",
    "        self.BayReg.add(x_new, y_new)\n",
    "        \n",
    "        return x_new, y_new\n",
    "    \n",
    "class RandomSearch():\n",
    "    d_MAX = 20\n",
    "    \n",
    "    def __init__(self, oracle:Oracle, d:int, seed:int=0):\n",
    "        assert isinstance(d, int) and 0<d<self.d_MAX, f\"Dimension `d` must be non-negative integer smaller than {self.d_MAX}\"\n",
    "        \n",
    "        self.oracle = oracle\n",
    "        self.d = d\n",
    "        self.seed = seed\n",
    "        \n",
    "        np.random.seed(self.seed)\n",
    "    \n",
    "    def update(self,):\n",
    "        '''\n",
    "        Sample random x (& noisy oracle function value) y\n",
    "        '''\n",
    "        x_new = np.random.binomial(n=1, p=0.5, size=self.d)\n",
    "        y_new = self.oracle.f(x=x_new)\n",
    "        return x_new, y_new\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "824e45fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SA' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [194]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sa1 \u001b[38;5;241m=\u001b[39m \u001b[43mSA\u001b[49m(oracle\u001b[38;5;241m=\u001b[39morc1, N\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, d\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m, Tfac\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m X, y \u001b[38;5;241m=\u001b[39m sa1\u001b[38;5;241m.\u001b[39mrun()\n\u001b[1;32m      4\u001b[0m y\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SA' is not defined"
     ]
    }
   ],
   "source": [
    "sa1 = SA(oracle=orc1, N=50, d=6, Tfac=0.8, mode='min')\n",
    "\n",
    "X, y = sa1.run()\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b490b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rndIndex = np.random.randint(self.d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837e22ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequentist Version\n",
    "# - - - - - - - - - - -\n",
    "# Problem\n",
    "d  = 10\n",
    "n  = 20\n",
    "n1 = 100\n",
    "\n",
    "#\n",
    "dictBOCS = {}\n",
    "dictRND = {}\n",
    "for j in range(100): \n",
    "    try:\n",
    "        # oracle\n",
    "        np.random.seed(16*j+600)\n",
    "        coef = np.random.normal(loc=0, scale=5, size=1+d+math.comb(d,2))\n",
    "        orc1 = Oracle(fun= lambda x: sum([coef[i] * x[i] for i in range(len(x)-1)]), sigma_2=3.0, N_total=100_000_000)\n",
    "\n",
    "        # D0\n",
    "        np.random.seed(j)\n",
    "        X = np.random.binomial(n=1,p=0.5, size=n*d).reshape((n,d))\n",
    "        y = orc1.f(X)\n",
    "\n",
    "        # BOCS\n",
    "        bocs = BOCS(oracle=orc1, N=200, B=10, T=2)\n",
    "        bocs.fit(X,y)\n",
    "\n",
    "        # Random Search\n",
    "        rndS = RandomSearch(oracle=orc1, d=d)\n",
    "\n",
    "        # SMAC\n",
    "\n",
    "        # L2S-DISCO\n",
    "\n",
    "        BocsfList = [0]\n",
    "        RndList = [0]\n",
    "        for i in range(n1):\n",
    "            # BOCS\n",
    "            x_, y_ = bocs.update()\n",
    "            BocsfList.append(min(y_, min(BocsfList)))\n",
    "\n",
    "            # Random Search\n",
    "            x_, y_ = rndS.update()\n",
    "            RndList.append(min(y_, min(RndList)))\n",
    "\n",
    "        # re-scale\n",
    "        y_MAX = max(max(BocsfList), max(RndList))\n",
    "        y_MIN = min(min(BocsfList), min(RndList))\n",
    "\n",
    "        # update\n",
    "        BocsfList = BocsfList / (y_MAX - y_MIN)\n",
    "        RndList = RndList / (y_MAX - y_MIN)\n",
    "\n",
    "        # update\n",
    "        dictBOCS[j] = BocsfList\n",
    "        dictRND[j]  = RndList\n",
    "    except:\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d532fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RANGE\n",
    "y_MAX = max(max(BocsfList), max(RndList))\n",
    "y_MIN = min(min(BocsfList), min(RndList))\n",
    "\n",
    "# update\n",
    "BocsfList = BocsfList / (y_MAX - y_MIN)\n",
    "RndList = RndList / (y_MAX - y_MIN)\n",
    "\n",
    "# Plot\n",
    "plt.plot(BocsfList / (y_MAX - y_MIN), color='darkblue')\n",
    "plt.plot(RndList / (y_MAX - y_MIN), color='orange')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f83df0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "fig,ax = plt.subplots()\n",
    "x_ = np.array(range(30, 131))\n",
    "  \n",
    "mult, inter = 100, 1\n",
    "q1,q2 = 0.35, 0.65\n",
    "q10, q90 = 0.25, 0.75\n",
    "    \n",
    "# mean\n",
    "y_mean  = ((np.array(pd.DataFrame(dictRND).quantile(0.5, axis=1)))+inter)*mult\n",
    "ax.plot(x_, y_mean, '--', color='tab:orange')\n",
    "\n",
    "#plt.plot(range(20, 20+len(mseDict[0])), [mse_**2 for mse_ in mse])\n",
    "plt.xticks()\n",
    "\n",
    "# compute upper lower\n",
    "y_lower = (np.array(pd.DataFrame(dictRND).quantile(q10, axis=1))+inter)*mult\n",
    "y_upper = (np.array(pd.DataFrame(dictRND).quantile(q90, axis=1))+inter)*mult\n",
    "        \n",
    "# plot our confidence band\n",
    "ax.fill_between(x_, y_lower, y_upper, alpha=0.2, color='tab:orange')\n",
    "\n",
    "# compute upper lower\n",
    "y_lower = (np.array(pd.DataFrame(dictRND).quantile(q1, axis=1))+inter)*mult\n",
    "y_upper = (np.array(pd.DataFrame(dictRND).quantile(q2, axis=1))+inter)*mult\n",
    "ax.fill_between(x_, y_lower, y_upper, alpha=0.2, color='tab:orange')\n",
    "\n",
    "# compute upper lower\n",
    "y_lower = (np.array(pd.DataFrame(dictBOCS).quantile(q10, axis=1))+inter)*mult\n",
    "y_upper = (np.array(pd.DataFrame(dictBOCS).quantile(q90, axis=1))+inter)*mult\n",
    "        \n",
    "# plot our confidence band\n",
    "ax.fill_between(x_, y_lower, y_upper, alpha=0.2, color='tab:blue')\n",
    "\n",
    "# compute upper lower\n",
    "y_mean  = (np.array(pd.DataFrame(dictBOCS).quantile(0.5, axis=1))+inter)*mult\n",
    "ax.plot(x_, y_mean, '--', color='tab:blue')\n",
    "y_lower = (np.array(pd.DataFrame(dictBOCS).quantile(q1, axis=1))+inter)*mult\n",
    "y_upper = (np.array(pd.DataFrame(dictBOCS).quantile(q2, axis=1))+inter)*mult\n",
    "ax.fill_between(x_, y_lower, y_upper, alpha=0.2, color='tab:blue')\n",
    "\n",
    "# ticks\n",
    "plt.suptitle('Effectiveness of BOCS', fontsize=16, y=0.99)\n",
    "plt.title(r'$p=56, \\; \\; \\Vert \\alpha \\Vert_{0}=p, \\; \\; n_{init}=30, \\;\\; \\sigma^{2}=3.0, \\; \\; N=100, \\; \\; B = 50, \\; T=4$', fontsize=10)\n",
    "plt.xlabel(r'$n$', fontsize=14)\n",
    "#plt.yscale(\"log\") \n",
    "plt.ylim(-10,100)\n",
    "plt.ylabel(r'$f(x_{t})$', fontsize=14)\n",
    "plt.legend({'Rnd.Search' : 'Random', 'BOCS-SDP' : 'BOCS-SDP'})\n",
    "#plt.axhline(y=0.06, linestyle='--', c='grey', linewidth=0.75, xmin=0.05, xmax=0.95)\n",
    "#plt.text(s=r'$\\sigma^{2} tr((X^{\\top} X )^{-1})$', y=0.06, x=-26, fontsize=8, c='#525252')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67a8c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SMAC():\n",
    "    d_MAX = 20\n",
    "    \n",
    "    def __init__(self, oracle:Oracle, d:int, seed:int=0):\n",
    "        assert isinstance(d, int) and 0<d<self.d_MAX, f\"Dimension `d` must be non-negative integer smaller than {self.d_MAX}\"\n",
    "        \n",
    "        self.oracle = Oracle\n",
    "        self.d = d\n",
    "        self.seed = seed\n",
    "        \n",
    "        np.random.seed(self.seed)\n",
    "    \n",
    "    def update(self,):\n",
    "        '''\n",
    "        Sample random x (& noisy oracle function value) y\n",
    "        '''\n",
    "        return np.random.binomial(n=1, p=0.5, size=self.d)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34727b02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f5fff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bo_opt import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eea092a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oracle\n",
    "coef = np.random.normal(loc=0, scale=5, size=1+d+math.comb(d,2))\n",
    "orc1 = Oracle(fun= lambda x: sum([coef[i] * x[i] for i in range(len(x)-1)]), \n",
    "              sigma_2=3.0, \n",
    "              N_total=100_000)\n",
    "\n",
    "# BOCS\n",
    "bocs = BOCS(oracle=orc1, N=200, B=10, T=2)\n",
    "\n",
    "# initial D_{0}\n",
    "X = np.random.binomial(n=1,p=0.5, size=n*d).reshape((n,d))\n",
    "y = orc1.f(X)\n",
    "\n",
    "# fit BOCS on initial data\n",
    "bocs.fit(X,y)\n",
    "\n",
    "# - let BOCS explore\n",
    "for k in range(10):\n",
    "    x_new, y_new = bocs.update()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
